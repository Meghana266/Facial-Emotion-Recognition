{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d785d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed1aa4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "data = pd.read_csv(\"fer2013.csv\")\n",
    "\n",
    "# Extract pixel values and emotions\n",
    "pixels = data['pixels'].apply(lambda x: np.array(x.split(), dtype=int))\n",
    "emotions = data['emotion']\n",
    "usage = data['Usage']  # Assuming there's a column named 'Usage'\n",
    "\n",
    "# Reshape the pixel arrays to 2D arrays\n",
    "pixels_2d = pixels.apply(lambda x: np.reshape(x, (48, 48)))\n",
    "\n",
    "# Reshape pixel values to be suitable for input to models\n",
    "X = np.array(pixels_2d.tolist())[:, None, :, :]\n",
    "\n",
    "# Split the data into training and testing sets based on the 'Usage' column\n",
    "X_train = X[usage == 'Training']\n",
    "y_train = emotions[usage == 'Training']\n",
    "\n",
    "X_test = X[usage == 'PrivateTest']\n",
    "y_test = emotions[usage == 'PrivateTest']\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8a14cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([28709, 1, 48, 48])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8571c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "# Create TensorDataset and DataLoader for training and test data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3801bd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48, 48]) tensor(4)\n"
     ]
    }
   ],
   "source": [
    "# Get a batch of images\n",
    "image_batch, label_batch = next(iter(train_loader))\n",
    "\n",
    "# Get a single image from the batch\n",
    "image, label = image_batch[0], label_batch[0]\n",
    "\n",
    "# View the batch shapes\n",
    "print(image.shape, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ff2158",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
    "    \n",
    "    Args:\n",
    "        in_channels (int): Number of color channels for the input images. Defaults to 1 for grayscale.\n",
    "        patch_size (int): Size of patches to convert input image into. Defaults to 4.\n",
    "        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
    "    \"\"\" \n",
    "    def __init__(self, \n",
    "                 in_channels: int = 1,  # Adjusted for grayscale images\n",
    "                 patch_size: int = 4,  # Adjusted patch size for 48x48 image\n",
    "                 embedding_dim: int = 16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # Create a layer to turn an image into patches\n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=embedding_dim,\n",
    "                                 kernel_size=patch_size,\n",
    "                                 stride=patch_size,\n",
    "                                 padding=0)\n",
    "\n",
    "        # Calculate the number of patches\n",
    "        self.num_patches = (48 // patch_size) ** 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Perform the forward pass\n",
    "        x_patched = self.patcher(x)  # Output shape: [batch_size, embedding_dim, num_patches, num_patches]\n",
    "        x_flattened = x_patched.flatten(2).transpose(1, 2)  # Flatten patches and transpose to [batch_size, num_patches, embedding_dim]\n",
    "        return x_flattened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca26188c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: torch.Size([1, 1, 48, 48])\n",
      "Output patch embedding shape: torch.Size([1, 144, 16])\n"
     ]
    }
   ],
   "source": [
    "# Let's test it on a single image\n",
    "patch_size = 4\n",
    "\n",
    "# Set seeds\n",
    "def set_seeds(seed: int = 42):\n",
    "    \"\"\"Sets random seeds for torch operations.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "# Assuming 'image' is your grayscale image\n",
    "image = torch.randn(1, 1, 48, 48)  # Assuming a single grayscale image of size 48x48\n",
    "\n",
    "# Create an instance of patch embedding layer\n",
    "patchify = PatchEmbedding(in_channels=1,  # Grayscale image has 1 channel\n",
    "                          patch_size=4,\n",
    "                          embedding_dim=16)\n",
    "\n",
    "# Pass a single image through\n",
    "print(f\"Input image shape: {image.shape}\")\n",
    "patch_embedded_image = patchify(image)\n",
    "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2857d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.6472,  0.4289, -0.4764,  ...,  0.1373,  1.3341, -0.4216],\n",
      "         [ 0.8679, -0.5651,  0.5903,  ..., -0.0661,  0.4061,  0.3041],\n",
      "         [-0.5880, -0.8610, -0.2638,  ...,  0.4933,  1.2292, -0.0019],\n",
      "         ...,\n",
      "         [-0.6974,  0.2374,  0.3840,  ..., -0.6328,  0.6245, -0.8937],\n",
      "         [-0.6851,  0.2170, -0.4709,  ...,  1.2021,  1.0993,  0.1823],\n",
      "         [-0.5813,  0.4791,  0.5753,  ...,  0.2897, -0.0834, -0.2073]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "Patch embedding shape: torch.Size([1, 144, 16]) -> [batch_size, number_of_patches, embedding_dimension]\n"
     ]
    }
   ],
   "source": [
    "print(patch_embedded_image) \n",
    "print(f\"Patch embedding shape: {patch_embedded_image.shape} -> [batch_size, number_of_patches, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6a8cc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor shape: torch.Size([1, 1, 48, 48])\n",
      "Input image with batch dimension shape: torch.Size([1, 1, 48, 48])\n",
      "Patching embedding shape: torch.Size([1, 144, 16])\n",
      "Class token embedding shape: torch.Size([1, 1, 16])\n",
      "Patch embedding with class token shape: torch.Size([1, 145, 16])\n",
      "Patch and position embedding shape: torch.Size([1, 145, 16])\n",
      "tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n",
      "         [-0.1089,  0.3940, -0.1063,  ...,  0.1169, -0.6876, -0.1364],\n",
      "         [-0.0791, -0.5349,  0.1944,  ..., -0.6428, -0.1077,  0.1746],\n",
      "         ...,\n",
      "         [ 0.6388,  0.6083, -0.0979,  ...,  0.4054,  0.7176,  0.3195],\n",
      "         [ 0.6316, -0.2121,  0.3735,  ...,  0.3643, -0.5640, -1.0396],\n",
      "         [-0.3404, -1.1636, -0.0343,  ..., -0.7795, -0.1963, -0.7098]]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# From start to positional encoding: All in 1 cell\n",
    "\n",
    "# Set seeds\n",
    "def set_seeds(seed: int = 42):\n",
    "    \"\"\"Sets random seeds for torch operations.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "# 1. Set patch size\n",
    "patch_size = 4\n",
    "\n",
    "# 2. Print shape of original image tensor and get the image dimensions\n",
    "print(f\"Image tensor shape: {image.shape}\")\n",
    "height, width = image.shape[2], image.shape[3]  # Adjusted for 48x48 image\n",
    "\n",
    "# 3. Get image tensor and add batch dimension\n",
    "x = image\n",
    "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
    "\n",
    "# 4. Create patch embedding layer\n",
    "patch_embedding_layer = PatchEmbedding(in_channels=1,  # Grayscale image has 1 channel\n",
    "                                       patch_size=patch_size,\n",
    "                                       embedding_dim=16)\n",
    "\n",
    "# 5. Pass image through patch embedding layer\n",
    "patch_embedding = patch_embedding_layer(x)\n",
    "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
    "\n",
    "# 6. Create class token embedding\n",
    "batch_size = patch_embedding.shape[0]\n",
    "embedding_dimension = patch_embedding.shape[-1]\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n",
    "                           requires_grad=True)  # make sure it's learnable\n",
    "print(f\"Class token embedding shape: {class_token.shape}\")\n",
    "\n",
    "# 7. Prepend class token embedding to patch embedding\n",
    "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
    "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
    "\n",
    "# 8. Create position embedding\n",
    "number_of_patches = int((height * width) / patch_size ** 2)\n",
    "position_embedding = nn.Parameter(torch.ones(1, number_of_patches + 1, embedding_dimension),\n",
    "                                  requires_grad=True)  # make sure it's learnable\n",
    "\n",
    "# 9. Add position embedding to patch embedding with class token\n",
    "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
    "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")\n",
    "\n",
    "print(patch_embedding_class_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd66d3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a class that inherits from nn.Module\n",
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
    "    \"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=16, # Hidden size D from Table 1 for ViT-Base\n",
    "                 num_heads:int=16, # Heads from Table 1 for ViT-Base\n",
    "                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n",
    "        super().__init__()\n",
    "        \n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        # 4. Create the Multi-Head Attention (MSA) layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True) # does our batch dimension come first?\n",
    "        \n",
    "    # 5. Create a forward() method to pass the data throguh the layers\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _ = self.multihead_attn(query=x, # query embeddings \n",
    "                                             key=x, # key embeddings\n",
    "                                             value=x, # value embeddings\n",
    "                                             need_weights=False) # do we need the weights or just the layer outputs?\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87d62889",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
    "    def __init__(self, embedding_dim=16, mlp_size=3072, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create the Norm layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim, out_features=mlp_size),\n",
    "            nn.GELU(),  # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size, out_features=embedding_dim),\n",
    "            nn.Dropout(p=dropout)  # \"Dropout, when used, is applied after every dense layer..\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aef488a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
    "    def __init__(self, embedding_dim=16, num_heads=16, mlp_size=3072, mlp_dropout=0.1, attn_dropout=0):\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Create MSA block (equation 2)\n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        \n",
    "        # 4. Create MLP block (equation 3)\n",
    "        self.mlp_block = MLPBlock(embedding_dim=embedding_dim,\n",
    "                                  mlp_size=mlp_size,\n",
    "                                  dropout=mlp_dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 6. Create residual connection for MSA block (add the input to the output)\n",
    "        x_msa = self.msa_block(x) + x \n",
    "        \n",
    "        # 7. Create residual connection for MLP block (add the input to the output)\n",
    "        x_mlp = self.mlp_block(x_msa) + x_msa \n",
    "        \n",
    "        return x_mlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "395afab2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
       "==================================================================================================================================\n",
       "TransformerEncoderBlock (TransformerEncoderBlock)  [1, 145, 16]         [1, 145, 16]         --                   True\n",
       "├─MultiheadSelfAttentionBlock (msa_block)          [1, 145, 16]         [1, 145, 16]         --                   True\n",
       "│    └─LayerNorm (layer_norm)                      [1, 145, 16]         [1, 145, 16]         32                   True\n",
       "│    └─MultiheadAttention (multihead_attn)         --                   [1, 145, 16]         1,088                True\n",
       "├─MLPBlock (mlp_block)                             [1, 145, 16]         [1, 145, 16]         --                   True\n",
       "│    └─LayerNorm (layer_norm)                      [1, 145, 16]         [1, 145, 16]         32                   True\n",
       "│    └─Sequential (mlp)                            [1, 145, 16]         [1, 145, 16]         --                   True\n",
       "│    │    └─Linear (0)                             [1, 145, 16]         [1, 145, 3072]       52,224               True\n",
       "│    │    └─GELU (1)                               [1, 145, 3072]       [1, 145, 3072]       --                   --\n",
       "│    │    └─Dropout (2)                            [1, 145, 3072]       [1, 145, 3072]       --                   --\n",
       "│    │    └─Linear (3)                             [1, 145, 3072]       [1, 145, 16]         49,168               True\n",
       "│    │    └─Dropout (4)                            [1, 145, 16]         [1, 145, 16]         --                   --\n",
       "==================================================================================================================================\n",
       "Total params: 102,544\n",
       "Trainable params: 102,544\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.10\n",
       "==================================================================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 3.62\n",
       "Params size (MB): 0.41\n",
       "Estimated Total Size (MB): 4.03\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the TransformerEncoderBlock\n",
    "transformer_encoder_block = TransformerEncoderBlock()\n",
    "\n",
    "# Print summary of the TransformerEncoderBlock\n",
    "from torchinfo import summary\n",
    "\n",
    "# Print an input and output summary of our Transformer Encoder (uncomment for full output)\n",
    "summary(model=transformer_encoder_block,\n",
    "        input_size=(1, 145, 16),  # (batch_size, num_patches, embedding_dimension)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "624c9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
    "    def __init__(self, img_size=48, in_channels=1, patch_size=4, num_transformer_layers=12,\n",
    "                 embedding_dim=16, mlp_size=3072, num_heads=16, attn_dropout=0, mlp_dropout=0.1,\n",
    "                 embedding_dropout=0.1, num_classes=7):\n",
    "        super().__init__()\n",
    "\n",
    "        # 3. Make sure the image size is divisible by the patch size\n",
    "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "\n",
    "        # 4. Calculate the number of patches\n",
    "        self.num_patches = (img_size * img_size) // (patch_size ** 2)\n",
    "\n",
    "        # 5. Create learnable class embedding\n",
    "        self.class_embedding = nn.Parameter(torch.randn(1, 1, embedding_dim), requires_grad=True)\n",
    "\n",
    "        # 6. Create learnable position embedding\n",
    "        self.position_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embedding_dim), requires_grad=True)\n",
    "\n",
    "        # 7. Create embedding dropout layer\n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "\n",
    "        # 8. Create patch embedding layer\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=in_channels, patch_size=patch_size, embedding_dim=embedding_dim)\n",
    "\n",
    "        # 9. Create Transformer Encoder blocks\n",
    "        self.transformer_encoder = nn.Sequential(\n",
    "            *[TransformerEncoderBlock(embedding_dim=embedding_dim, num_heads=num_heads,\n",
    "                                      mlp_size=mlp_size, mlp_dropout=mlp_dropout,\n",
    "                                      attn_dropout=attn_dropout) for _ in range(num_transformer_layers)]\n",
    "        )\n",
    "\n",
    "        # 10. Create classifier head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 12. Get batch size\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # 13. Create class token embedding\n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1)\n",
    "        \n",
    "        # 14. Create patch embedding\n",
    "        x = self.patch_embedding(x)\n",
    "        \n",
    "        x = torch.cat((class_token, x), dim=1)  # Concatenate along dimension 1\n",
    "\n",
    "        # 16. Add position embedding to patch embedding\n",
    "        x = self.position_embedding + x\n",
    "\n",
    "        # 17. Run embedding dropout\n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        # 19. Pass patch, position, and class embedding through transformer encoder layers\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        # 21. Put logit through the classifier\n",
    "        x = self.classifier(x[:,0])\n",
    "\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7c60b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = ViT(num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77723a11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Batch 100/449:\n",
      "  Loss: 1.8893, Accuracy: 0.1875\n",
      "Epoch 1/2, Batch 200/449:\n",
      "  Loss: 1.8770, Accuracy: 0.2188\n",
      "Epoch 1/2, Batch 300/449:\n",
      "  Loss: 1.8435, Accuracy: 0.3125\n",
      "Epoch 1/2, Batch 400/449:\n",
      "  Loss: 1.8785, Accuracy: 0.1875\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Calculate average loss and accuracy for the epoch\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m     63\u001b[0m epoch_accuracy \u001b[38;5;241m=\u001b[39m correct_predictions \u001b[38;5;241m/\u001b[39m total_predictions\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Print epoch statistics\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper \n",
    "optimizer = Adam(params=vit.parameters(), \n",
    "                 lr=3e-3,  # Base LR from Table 3 for ViT-* ImageNet-1k\n",
    "                 betas=(0.9, 0.999),  # Default values mentioned in the ViT paper section 4.1 (Training & Fine-tuning)\n",
    "                 weight_decay=0.3)  # From the ViT paper section 4.1 (Training & Fine-tuning) and Table 3 for ViT-* ImageNet-1k\n",
    "\n",
    "# Setup the loss function for multi-class classification\n",
    "loss_fn = F.cross_entropy\n",
    "\n",
    "# Set the seeds\n",
    "set_seeds()\n",
    "\n",
    "# Set number of epochs\n",
    "num_epochs = 2\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Set model to training mode\n",
    "    vit.train()\n",
    "    \n",
    "    # Initialize variables for tracking loss and accuracy\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Iterate over training data\n",
    "    for batch_idx,(inputs, labels) in enumerate(train_loader):\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = vit(inputs)\n",
    "        \n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "        # Print batch statistics if batch number is a multiple of 100\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            batch_loss = loss.item()\n",
    "            batch_accuracy = (predicted == labels).sum().item() / labels.size(0)\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(train_loader)}:\")\n",
    "            print(f\"  Loss: {batch_loss:.4f}, Accuracy: {batch_accuracy:.4f}\")\n",
    "    \n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9de1c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.8541, Test Accuracy: 0.2449\n"
     ]
    }
   ],
   "source": [
    "# Optionally, you can evaluate the model on the test data after training\n",
    "# Set model to evaluation mode\n",
    "vit.eval()\n",
    "\n",
    "# Initialize variables for tracking loss and accuracy on test data\n",
    "test_running_loss = 0.0\n",
    "test_correct_predictions = 0\n",
    "test_total_predictions = 0\n",
    "\n",
    "# Iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = vit(inputs)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = F.cross_entropy(outputs, labels)\n",
    "    \n",
    "    # Update running loss\n",
    "    test_running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    test_correct_predictions += (predicted == labels).sum().item()\n",
    "    test_total_predictions += labels.size(0)\n",
    "\n",
    "# Calculate average loss and accuracy on test data\n",
    "test_loss = test_running_loss / len(test_loader.dataset)\n",
    "test_accuracy = test_correct_predictions / test_total_predictions\n",
    "\n",
    "# Print test statistics\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "acb058fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(vit.state_dict(), 'vit_model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
